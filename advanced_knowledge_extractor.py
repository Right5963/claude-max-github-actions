#!/usr/bin/env python3
"""
é«˜åº¦çŸ¥è­˜æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
==================
æŠ½è±¡çš„å†…å®¹ â†’ å®Ÿç”¨çš„æ´å¯Ÿã¸ã®é©å‘½
"""

import os
import json
import re
import subprocess
from pathlib import Path
from datetime import datetime
from collections import defaultdict

class AdvancedKnowledgeExtractor:
    def __init__(self):
        self.work_dir = "/mnt/c/Claude Code/tool"
        
        # é–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºç”¨ã®è©³ç´°è¦å‰‡
        self.pattern_rules = {
            'refactoring': {
                'file_patterns': [r'.*\.py$', r'.*\.js$', r'.*\.ts$'],
                'content_patterns': [
                    r'def\s+\w+\s*\([^)]*\)\s*:',  # æ–°ã—ã„é–¢æ•°å®šç¾©
                    r'class\s+\w+\s*[\(:]',        # æ–°ã—ã„ã‚¯ãƒ©ã‚¹å®šç¾©
                    r'import\s+\w+',               # æ–°ã—ã„import
                    r'from\s+\w+\s+import',        # from import
                ],
                'deletion_patterns': [
                    r'#.*TODO.*',                  # TODOå‰Šé™¤
                    r'print\s*\(',                 # printæ–‡å‰Šé™¤
                    r'console\.log\s*\(',          # console.logå‰Šé™¤
                ]
            },
            'optimization': {
                'keywords': ['optimize', 'performance', 'speed', 'fast', 'cache', 'async', 'parallel'],
                'file_types': ['.py', '.js', '.ts', '.sql']
            },
            'bug_fix': {
                'keywords': ['fix', 'bug', 'error', 'exception', 'crash', 'fail'],
                'patterns': [
                    r'try\s*:.*except',            # ä¾‹å¤–å‡¦ç†è¿½åŠ 
                    r'if\s+.*\s+is\s+None',        # None ãƒã‚§ãƒƒã‚¯
                    r'isinstance\s*\(',            # å‹ãƒã‚§ãƒƒã‚¯
                ]
            },
            'feature_addition': {
                'keywords': ['add', 'new', 'feature', 'implement', 'create'],
                'indicators': [
                    'new_files_ratio',             # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«æ¯”ç‡
                    'new_functions_count',         # æ–°è¦é–¢æ•°æ•°
                    'api_endpoints',               # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
                ]
            },
            'security_improvement': {
                'keywords': ['security', 'auth', 'permission', 'validate', 'sanitize'],
                'patterns': [
                    r'password.*hash',
                    r'token.*verify',
                    r'input.*validate',
                    r'escape.*html',
                ]
            }
        }
        
        # ã‚³ãƒ¼ãƒ‰å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.quality_metrics = {
            'complexity': ['cyclomatic', 'cognitive', 'nesting'],
            'maintainability': ['readability', 'modularity', 'documentation'],
            'testability': ['coverage', 'mocking', 'isolation']
        }
    
    def analyze_commit_changes(self, commit_hash="HEAD"):
        """ã‚³ãƒŸãƒƒãƒˆã®å¤‰æ›´ã‚’è©³ç´°åˆ†æ"""
        print(f"ğŸ” é«˜åº¦åˆ†æé–‹å§‹: {commit_hash}")
        
        # 1. ã‚³ãƒŸãƒƒãƒˆæƒ…å ±å–å¾—
        commit_info = self.get_commit_info(commit_hash)
        
        # 2. å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        file_changes = self.analyze_file_changes(commit_hash)
        
        # 3. ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        code_patterns = self.detect_code_patterns(file_changes)
        
        # 4. å“è³ªå½±éŸ¿åˆ†æ
        quality_impact = self.analyze_quality_impact(file_changes)
        
        # 5. é–‹ç™ºæ´å¯Ÿç”Ÿæˆ
        insights = self.generate_development_insights(
            commit_info, file_changes, code_patterns, quality_impact
        )
        
        return {
            'commit_info': commit_info,
            'file_changes': file_changes,
            'code_patterns': code_patterns,
            'quality_impact': quality_impact,
            'insights': insights
        }
    
    def get_commit_info(self, commit_hash):
        """è©³ç´°ãªã‚³ãƒŸãƒƒãƒˆæƒ…å ±ã‚’å–å¾—"""
        try:
            # ã‚³ãƒŸãƒƒãƒˆæƒ…å ±
            result = subprocess.run([
                'git', 'show', '--format=%H%n%s%n%an%n%ad%n%B', '--name-status', commit_hash
            ], cwd=self.work_dir, capture_output=True, text=True)
            
            lines = result.stdout.strip().split('\n')
            if len(lines) < 4:
                return {}
            
            return {
                'hash': lines[0],
                'subject': lines[1],
                'author': lines[2],
                'date': lines[3],
                'message': '\n'.join(lines[4:lines.index('') if '' in lines[4:] else len(lines)]),
                'file_status': [line for line in lines if '\t' in line]
            }
        except Exception as e:
            print(f"âš ï¸ ã‚³ãƒŸãƒƒãƒˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def analyze_file_changes(self, commit_hash):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã®è©³ç´°åˆ†æ"""
        try:
            # å¤‰æ›´çµ±è¨ˆ
            result = subprocess.run([
                'git', 'show', '--stat', commit_hash
            ], cwd=self.work_dir, capture_output=True, text=True)
            
            stat_lines = result.stdout.strip().split('\n')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥å¤‰æ›´è©³ç´°
            file_details = []
            changes_summary = {'additions': 0, 'deletions': 0, 'files': 0}
            
            for line in stat_lines:
                if '|' in line and ('+' in line or '-' in line):
                    parts = line.split('|')
                    if len(parts) >= 2:
                        filename = parts[0].strip()
                        change_info = parts[1].strip()
                        
                        # å¤‰æ›´æ•°ã®æŠ½å‡º
                        additions = change_info.count('+')
                        deletions = change_info.count('-')
                        
                        file_details.append({
                            'filename': filename,
                            'additions': additions,
                            'deletions': deletions,
                            'total_changes': additions + deletions,
                            'file_type': Path(filename).suffix,
                            'change_type': self.classify_change_type(filename, additions, deletions)
                        })
                        
                        changes_summary['additions'] += additions
                        changes_summary['deletions'] += deletions
                        changes_summary['files'] += 1
            
            return {
                'summary': changes_summary,
                'files': file_details,
                'file_types': self.analyze_file_types(file_details),
                'change_distribution': self.analyze_change_distribution(file_details)
            }
            
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def classify_change_type(self, filename, additions, deletions):
        """å¤‰æ›´ã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        if deletions == 0:
            return "new_file"
        elif additions == 0:
            return "file_deletion"
        elif additions > deletions * 2:
            return "major_addition"
        elif deletions > additions * 2:
            return "major_reduction"
        else:
            return "modification"
    
    def analyze_file_types(self, file_details):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ"""
        type_stats = defaultdict(lambda: {'count': 0, 'additions': 0, 'deletions': 0})
        
        for file in file_details:
            file_type = file['file_type'] or 'no_extension'
            type_stats[file_type]['count'] += 1
            type_stats[file_type]['additions'] += file['additions']
            type_stats[file_type]['deletions'] += file['deletions']
        
        return dict(type_stats)
    
    def analyze_change_distribution(self, file_details):
        """å¤‰æ›´åˆ†å¸ƒã®åˆ†æ"""
        if not file_details:
            return {}
        
        total_changes = sum(f['total_changes'] for f in file_details)
        
        # æœ€ã‚‚å¤‰æ›´ã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«
        most_changed = max(file_details, key=lambda f: f['total_changes'])
        
        # å¤‰æ›´ã®é›†ä¸­åº¦
        concentration = most_changed['total_changes'] / total_changes if total_changes > 0 else 0
        
        return {
            'most_changed_file': most_changed,
            'change_concentration': concentration,
            'average_changes_per_file': total_changes / len(file_details),
            'change_types': [f['change_type'] for f in file_details]
        }
    
    def detect_code_patterns(self, file_changes):
        """ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        patterns_detected = []
        
        for file_info in file_changes.get('files', []):
            filename = file_info['filename']
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å¤‰æ›´ã‚’åˆ†æ
            patterns = self.analyze_file_content_patterns(filename, file_info)
            patterns_detected.extend(patterns)
        
        return self.consolidate_patterns(patterns_detected)
    
    def analyze_file_content_patterns(self, filename, file_info):
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã«ã‚ˆã‚‹åˆ†é¡
        file_ext = Path(filename).suffix
        
        # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
        if file_info['change_type'] == 'modification' and file_ext in ['.py', '.js', '.ts']:
            if file_info['deletions'] > 0 and file_info['additions'] > 0:
                patterns.append({
                    'type': 'refactoring',
                    'confidence': 0.7,
                    'evidence': f"{file_info['deletions']}è¡Œå‰Šé™¤ã€{file_info['additions']}è¡Œè¿½åŠ ",
                    'description': f"{filename}ã§ã‚³ãƒ¼ãƒ‰æ§‹é€ ã®æ”¹å–„"
                })
        
        # æ–°æ©Ÿèƒ½è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³
        if file_info['change_type'] == 'new_file':
            patterns.append({
                'type': 'feature_addition',
                'confidence': 0.9,
                'evidence': f"æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«: {filename}",
                'description': f"{file_ext}ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹æ–°æ©Ÿèƒ½å®Ÿè£…"
            })
        
        # æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
        if file_info['additions'] > file_info['deletions'] and 'optimized' in filename.lower():
            patterns.append({
                'type': 'optimization',
                'confidence': 0.8,
                'evidence': f"æœ€é©åŒ–ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«: {filename}",
                'description': "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å®Ÿè£…"
            })
        
        return patterns
    
    def consolidate_patterns(self, patterns_detected):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±åˆã¨å„ªå…ˆé †ä½ä»˜ã‘"""
        pattern_groups = defaultdict(list)
        
        for pattern in patterns_detected:
            pattern_groups[pattern['type']].append(pattern)
        
        consolidated = {}
        for pattern_type, pattern_list in pattern_groups.items():
            # ä¿¡é ¼åº¦ã®å¹³å‡
            avg_confidence = sum(p['confidence'] for p in pattern_list) / len(pattern_list)
            
            # è¨¼æ‹ ã®çµ±åˆ
            evidence = [p['evidence'] for p in pattern_list]
            descriptions = [p['description'] for p in pattern_list]
            
            consolidated[pattern_type] = {
                'confidence': avg_confidence,
                'count': len(pattern_list),
                'evidence': evidence,
                'descriptions': descriptions
            }
        
        return consolidated
    
    def analyze_quality_impact(self, file_changes):
        """å“è³ªã¸ã®å½±éŸ¿åˆ†æ"""
        impact = {
            'complexity_change': 'neutral',
            'maintainability_change': 'neutral',
            'testability_change': 'neutral',
            'overall_quality_trend': 'stable'
        }
        
        if not file_changes:
            return impact
        
        summary = file_changes.get('summary', {})
        files = file_changes.get('files', [])
        
        # è¤‡é›‘æ€§ã¸ã®å½±éŸ¿
        if summary.get('additions', 0) > summary.get('deletions', 0) * 1.5:
            impact['complexity_change'] = 'increased'
        elif summary.get('deletions', 0) > summary.get('additions', 0) * 1.5:
            impact['complexity_change'] = 'decreased'
        
        # ä¿å®ˆæ€§ã¸ã®å½±éŸ¿
        test_files = [f for f in files if 'test' in f['filename'].lower()]
        doc_files = [f for f in files if f['file_type'] in ['.md', '.txt', '.rst']]
        
        if test_files or doc_files:
            impact['maintainability_change'] = 'improved'
        
        # ãƒ†ã‚¹ãƒˆå¯èƒ½æ€§ã¸ã®å½±éŸ¿
        if test_files:
            impact['testability_change'] = 'improved'
        
        # å…¨ä½“çš„ãªå“è³ªãƒˆãƒ¬ãƒ³ãƒ‰
        improvement_factors = sum([
            impact['complexity_change'] == 'decreased',
            impact['maintainability_change'] == 'improved',
            impact['testability_change'] == 'improved'
        ])
        
        if improvement_factors >= 2:
            impact['overall_quality_trend'] = 'improving'
        elif impact['complexity_change'] == 'increased':
            impact['overall_quality_trend'] = 'declining'
        
        return impact
    
    def generate_development_insights(self, commit_info, file_changes, code_patterns, quality_impact):
        """å®Ÿç”¨çš„ãªé–‹ç™ºæ´å¯Ÿã‚’ç”Ÿæˆ"""
        insights = []
        
        # 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿ
        if 'optimization' in code_patterns:
            opt_pattern = code_patterns['optimization']
            insights.append(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: {opt_pattern['count']}ç®‡æ‰€ã§æ”¹å–„å®Ÿè£… - {', '.join(opt_pattern['evidence'])}")
        
        # 2. ã‚³ãƒ¼ãƒ‰å“è³ªæ´å¯Ÿ
        if quality_impact['overall_quality_trend'] == 'improving':
            insights.append(f"ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Š: ä¿å®ˆæ€§({quality_impact['maintainability_change']}) + ãƒ†ã‚¹ãƒˆæ€§({quality_impact['testability_change']})ã®æ”¹å–„")
        
        # 3. é–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³æ´å¯Ÿ
        if 'refactoring' in code_patterns:
            refactor = code_patterns['refactoring']
            insights.append(f"ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿæ–½: {refactor['count']}ãƒ•ã‚¡ã‚¤ãƒ«ã§æ§‹é€ æ”¹å–„ - å¯èª­æ€§ãƒ»ä¿å®ˆæ€§ã®å‘ä¸Š")
        
        # 4. æ–°æ©Ÿèƒ½æ´å¯Ÿ
        if 'feature_addition' in code_patterns:
            feature = code_patterns['feature_addition']
            insights.append(f"æ–°æ©Ÿèƒ½é–‹ç™º: {feature['count']}å€‹ã®æ–°ã—ã„æ©Ÿèƒ½ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¿½åŠ  - ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ")
        
        # 5. å¤‰æ›´è¦æ¨¡æ´å¯Ÿ
        if file_changes and file_changes.get('summary'):
            summary = file_changes['summary']
            if summary.get('files', 0) > 10:
                insights.append(f"å¤§è¦æ¨¡å¤‰æ›´: {summary['files']}ãƒ•ã‚¡ã‚¤ãƒ«ã€{summary['additions']}è¡Œè¿½åŠ  - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¬ãƒ™ãƒ«ã®æ”¹å–„")
            elif summary.get('additions', 0) > 100:
                insights.append(f"æ©Ÿèƒ½æ‹¡å¼µ: {summary['additions']}è¡Œã®æ–°è¦ã‚³ãƒ¼ãƒ‰ - æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®å¤§å¹…å¼·åŒ–")
        
        # 6. æŠ€è¡“è² å‚µæ´å¯Ÿ
        if file_changes and file_changes.get('summary'):
            summary = file_changes['summary']
            if summary.get('deletions', 0) > summary.get('additions', 0):
                insights.append(f"æŠ€è¡“è² å‚µè§£æ¶ˆ: {summary['deletions']}è¡Œå‰Šé™¤ - ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ•´ç†ãƒ»æœ€é©åŒ–")
        
        # 7. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ´å¯Ÿã®å›é¿
        if not insights:
            # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ´å¯Ÿ
            if commit_info and commit_info.get('subject'):
                subject = commit_info['subject']
                if 'Auto-commit' in subject:
                    insights.append(f"é–‹ç™ºåŠ¹ç‡åŒ–: è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹ç¶™ç¶šçš„ãªå¤‰æ›´ç®¡ç†ã®å®Ÿç¾")
                else:
                    insights.append(f"é–‹ç™ºæ¨é€²: {subject}ã«ã‚ˆã‚‹æ©Ÿèƒ½å‘ä¸Šãƒ»å•é¡Œè§£æ±ºã®å®Ÿè£…")
        
        return insights

# ãƒ¡ã‚¤ãƒ³ä½¿ç”¨ä¾‹
def main():
    extractor = AdvancedKnowledgeExtractor()
    
    # æœ€æ–°ã‚³ãƒŸãƒƒãƒˆã‚’åˆ†æ
    analysis = extractor.analyze_commit_changes("HEAD")
    
    print("ğŸ§  é«˜åº¦çŸ¥è­˜æŠ½å‡ºçµæœ:")
    print("=" * 50)
    
    if analysis['insights']:
        print("ğŸ’¡ é–‹ç™ºæ´å¯Ÿ:")
        for i, insight in enumerate(analysis['insights'], 1):
            print(f"  {i}. {insight}")
    
    if analysis['code_patterns']:
        print("\nğŸ” æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³:")
        for pattern_type, pattern_data in analysis['code_patterns'].items():
            print(f"  {pattern_type}: ä¿¡é ¼åº¦{pattern_data['confidence']:.1f} ({pattern_data['count']}ä»¶)")
    
    if analysis['quality_impact']:
        print("\nğŸ“Š å“è³ªã¸ã®å½±éŸ¿:")
        impact = analysis['quality_impact']
        print(f"  å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ³ãƒ‰: {impact['overall_quality_trend']}")
        print(f"  è¤‡é›‘æ€§: {impact['complexity_change']}")
        print(f"  ä¿å®ˆæ€§: {impact['maintainability_change']}")

if __name__ == "__main__":
    main()