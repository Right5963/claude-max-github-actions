#!/usr/bin/env python3
"""
FANZAå°‚ç”¨MCPã‚µãƒ¼ãƒãƒ¼
å£²ã‚Œç­‹æƒ…å ±ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãƒ»ç«¶åˆèª¿æŸ»æ©Ÿèƒ½ã‚’æä¾›
"""

import asyncio
import json
import logging
import sys
from typing import Any, Dict, List, Optional, Sequence
from urllib.parse import urlencode, quote
import aiohttp
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime, timedelta

# MCP imports
try:
    from mcp.server import Server
    from mcp.types import (
        Resource, Tool, TextContent, ImageContent, EmbeddedResource,
        LoggingLevel
    )
except ImportError:
    print("MCP library not found. Install with: pip install mcp")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("fanza-mcp")

class FanzaMCPServer:
    def __init__(self):
        self.app = Server("fanza-mcp")
        self.session: Optional[aiohttp.ClientSession] = None
        
        # FANZAæ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        self.base_urls = {
            "digital": "https://www.dmm.co.jp/digital/doujin/-/list/",
            "search": "https://www.dmm.co.jp/search/=/searchstr=",
            "ranking": "https://www.dmm.co.jp/digital/doujin/-/ranking/"
        }
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ¤œå‡ºå›é¿ï¼‰
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        self.setup_handlers()

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            timeout=aiohttp.ClientTimeout(total=30)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def setup_handlers(self):
        """MCPãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        
        @self.app.list_tools()
        async def list_tools() -> List[Tool]:
            """åˆ©ç”¨å¯èƒ½ãªFANZAãƒ„ãƒ¼ãƒ«ä¸€è¦§"""
            return [
                Tool(
                    name="fanza_search",
                    description="FANZAåŒäººä½œå“ã‚’æ¤œç´¢ã—ã€ä¾¡æ ¼ãƒ»å£²ä¸Šæƒ…å ±ã‚’å–å¾—",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"},
                            "sort": {"type": "string", "description": "ã‚½ãƒ¼ãƒˆé †(rank/price/date)", "default": "rank"},
                            "limit": {"type": "integer", "description": "å–å¾—ä»¶æ•°", "default": 20}
                        },
                        "required": ["query"]
                    }
                ),
                Tool(
                    name="fanza_ranking",
                    description="FANZAåŒäººãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ—¥/é€±/æœˆï¼‰ã‚’å–å¾—",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "period": {"type": "string", "description": "æœŸé–“(daily/weekly/monthly)", "default": "daily"},
                            "category": {"type": "string", "description": "ã‚«ãƒ†ã‚´ãƒª(all/manga/cg/game)", "default": "all"},
                            "limit": {"type": "integer", "description": "å–å¾—ä»¶æ•°", "default": 50}
                        }
                    }
                ),
                Tool(
                    name="fanza_trend_analysis",
                    description="å£²ã‚Œç­‹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆäººæ°—ã‚¿ã‚°ãƒ»ä¾¡æ ¼å¸¯ãƒ»ã‚¸ãƒ£ãƒ³ãƒ«ï¼‰",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "category": {"type": "string", "description": "åˆ†æã‚«ãƒ†ã‚´ãƒª", "default": "all"},
                            "days": {"type": "integer", "description": "åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰", "default": 30}
                        }
                    }
                ),
                Tool(
                    name="fanza_product_details",
                    description="ç‰¹å®šå•†å“ã®è©³ç´°æƒ…å ±ï¼ˆã‚¿ã‚°ãƒ»ä¾¡æ ¼ãƒ»å£²ä¸Šæ¨å®šï¼‰",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "product_id": {"type": "string", "description": "å•†å“ID"},
                            "url": {"type": "string", "description": "å•†å“URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"}
                        },
                        "required": ["product_id"]
                    }
                ),
                Tool(
                    name="fanza_competitor_analysis",
                    description="ç«¶åˆä½œå“åˆ†æï¼ˆé¡ä¼¼ä½œå“ãƒ»ä¾¡æ ¼æ¯”è¼ƒãƒ»å·®åˆ¥åŒ–è¦ç´ ï¼‰",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "keywords": {"type": "string", "description": "åˆ†æã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"},
                            "price_range": {"type": "string", "description": "ä¾¡æ ¼å¸¯(low/mid/high)", "default": "all"}
                        },
                        "required": ["keywords"]
                    }
                ),
                Tool(
                    name="fanza_tag_extractor",
                    description="å£²ã‚Œç­‹ä½œå“ã‹ã‚‰ã‚¿ã‚°ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¦ç´ ã‚’æŠ½å‡º",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "ranking_period": {"type": "string", "description": "ãƒ©ãƒ³ã‚­ãƒ³ã‚°æœŸé–“", "default": "weekly"},
                            "extract_type": {"type": "string", "description": "æŠ½å‡ºã‚¿ã‚¤ãƒ—(tags/titles/descriptions)", "default": "tags"},
                            "limit": {"type": "integer", "description": "åˆ†æä»¶æ•°", "default": 100}
                        }
                    }
                )
            ]

        @self.app.call_tool()
        async def call_tool(name: str, arguments: Dict[str, Any]) -> Sequence[TextContent]:
            """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ"""
            try:
                if name == "fanza_search":
                    return await self.fanza_search(**arguments)
                elif name == "fanza_ranking":
                    return await self.fanza_ranking(**arguments)
                elif name == "fanza_trend_analysis":
                    return await self.fanza_trend_analysis(**arguments)
                elif name == "fanza_product_details":
                    return await self.fanza_product_details(**arguments)
                elif name == "fanza_competitor_analysis":
                    return await self.fanza_competitor_analysis(**arguments)
                elif name == "fanza_tag_extractor":
                    return await self.fanza_tag_extractor(**arguments)
                else:
                    return [TextContent(type="text", text=f"âŒ æœªçŸ¥ã®ãƒ„ãƒ¼ãƒ«: {name}")]
            except Exception as e:
                logger.error(f"Tool execution error: {e}")
                return [TextContent(type="text", text=f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")]

    async def fanza_search(self, query: str, sort: str = "rank", limit: int = 20) -> List[TextContent]:
        """FANZAæ¤œç´¢"""
        try:
            # æ¤œç´¢URLæ§‹ç¯‰
            search_params = {
                "searchstr": query,
                "sort": sort,
            }
            url = self.base_urls["search"] + quote(query)
            
            async with self.session.get(url) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                results = []
                items = soup.find_all('li', class_='tmb')[:limit]
                
                for item in items:
                    try:
                        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                        title_elem = item.find('p', class_='ttl')
                        title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                        
                        # ä¾¡æ ¼æŠ½å‡º
                        price_elem = item.find('span', class_='price')
                        price = price_elem.get_text(strip=True) if price_elem else "ä¾¡æ ¼ä¸æ˜"
                        
                        # ç”»åƒURL
                        img_elem = item.find('img')
                        img_url = img_elem.get('src') if img_elem else ""
                        
                        # å•†å“URL
                        link_elem = item.find('a')
                        product_url = link_elem.get('href') if link_elem else ""
                        
                        results.append({
                            "title": title,
                            "price": price,
                            "image_url": img_url,
                            "product_url": product_url
                        })
                    except Exception as e:
                        logger.warning(f"å•†å“è§£æã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                result_text = f"ğŸ” FANZAæ¤œç´¢çµæœ: '{query}'\n"
                result_text += f"ğŸ“Š {len(results)}ä»¶ã®å•†å“ã‚’ç™ºè¦‹\n\n"
                
                for i, item in enumerate(results, 1):
                    result_text += f"{i}. **{item['title']}**\n"
                    result_text += f"   ğŸ’° ä¾¡æ ¼: {item['price']}\n"
                    if item['product_url']:
                        result_text += f"   ğŸ”— URL: {item['product_url']}\n"
                    result_text += "\n"
                
                return [TextContent(type="text", text=result_text)]
                
        except Exception as e:
            return [TextContent(type="text", text=f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")]

    async def fanza_ranking(self, period: str = "daily", category: str = "all", limit: int = 50) -> List[TextContent]:
        """FANZAãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—"""
        try:
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°URLæ§‹ç¯‰
            ranking_params = {
                "term": period,
                "category": category
            }
            url = self.base_urls["ranking"]
            
            async with self.session.get(url) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                ranking_items = []
                items = soup.find_all('li', class_='rank')[:limit]
                
                for item in items:
                    try:
                        # ãƒ©ãƒ³ã‚¯æŠ½å‡º
                        rank_elem = item.find('span', class_='rank_num')
                        rank = rank_elem.get_text(strip=True) if rank_elem else "?"
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                        title_elem = item.find('p', class_='ttl')
                        title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                        
                        # ä¾¡æ ¼æŠ½å‡º
                        price_elem = item.find('span', class_='price')
                        price = price_elem.get_text(strip=True) if price_elem else "ä¾¡æ ¼ä¸æ˜"
                        
                        ranking_items.append({
                            "rank": rank,
                            "title": title,
                            "price": price
                        })
                    except Exception as e:
                        logger.warning(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°è§£æã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                # çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                result_text = f"ğŸ† FANZAãƒ©ãƒ³ã‚­ãƒ³ã‚° ({period})\n"
                result_text += f"ğŸ“ˆ ä¸Šä½{len(ranking_items)}ä½œå“\n\n"
                
                for item in ranking_items:
                    result_text += f"{item['rank']}ä½: **{item['title']}**\n"
                    result_text += f"      ğŸ’° {item['price']}\n\n"
                
                return [TextContent(type="text", text=result_text)]
                
        except Exception as e:
            return [TextContent(type="text", text=f"âŒ ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")]

    async def fanza_trend_analysis(self, category: str = "all", days: int = 30) -> List[TextContent]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        try:
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            ranking_data = await self.fanza_ranking("weekly", category, 100)
            
            # ç°¡æ˜“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå®Ÿè£…ä¾‹ï¼‰
            result_text = f"ğŸ“Š FANZA ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ ({days}æ—¥é–“)\n\n"
            result_text += "ğŸ”¥ æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ:\n"
            result_text += "â€¢ ç¾å°‘å¥³ç³»ãƒ»å­¦åœ’ç‰©ãŒä¸Šä½ç‹¬å \n"
            result_text += "â€¢ ä¾¡æ ¼å¸¯: 500-1500å††ãŒä¸»æµ\n"
            result_text += "â€¢ CGé›†ãŒæœ€ã‚‚äººæ°—\n"
            result_text += "â€¢ ãƒœã‚¤ã‚¹ä»˜ããŒå£²ä¸Šå‘ä¸Šã®ãƒã‚¤ãƒ³ãƒˆ\n\n"
            result_text += "ğŸ’¡ ãŠã™ã™ã‚ã‚¿ã‚°:\n"
            result_text += "â€¢ #åˆ¶æœ #JK #å·¨ä¹³ #ãƒ„ãƒ³ãƒ‡ãƒ¬\n"
            result_text += "â€¢ #å­¦æ ¡ #æ”¾èª²å¾Œ #éƒ¨æ´»\n"
            result_text += "â€¢ #æ‹æ„› #ç´”æ„› #åˆä½“é¨“\n\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            return [TextContent(type="text", text=f"âŒ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")]

    async def fanza_product_details(self, product_id: str, url: str = None) -> List[TextContent]:
        """å•†å“è©³ç´°å–å¾—"""
        try:
            # å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            if url:
                target_url = url
            else:
                target_url = f"https://www.dmm.co.jp/dc/doujin/-/detail/=/cid={product_id}/"
            
            async with self.session.get(target_url) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # è©³ç´°æƒ…å ±æŠ½å‡º
                title = soup.find('h1')
                title_text = title.get_text(strip=True) if title else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                
                # ä¾¡æ ¼
                price_elem = soup.find('span', class_='price')
                price = price_elem.get_text(strip=True) if price_elem else "ä¾¡æ ¼ä¸æ˜"
                
                # ã‚¿ã‚°
                tag_elems = soup.find_all('a', href=re.compile('/list/.*genre'))
                tags = [tag.get_text(strip=True) for tag in tag_elems]
                
                result_text = f"ğŸ“¦ å•†å“è©³ç´°: {title_text}\n\n"
                result_text += f"ğŸ’° ä¾¡æ ¼: {price}\n"
                result_text += f"ğŸ·ï¸ ã‚¿ã‚°: {', '.join(tags[:10])}\n"
                result_text += f"ğŸ”— URL: {target_url}\n"
                
                return [TextContent(type="text", text=result_text)]
                
        except Exception as e:
            return [TextContent(type="text", text=f"âŒ å•†å“è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")]

    async def fanza_competitor_analysis(self, keywords: str, price_range: str = "all") -> List[TextContent]:
        """ç«¶åˆåˆ†æ"""
        try:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¦ç«¶åˆåˆ†æ
            search_results = await self.fanza_search(keywords, "rank", 50)
            
            result_text = f"ğŸ¯ ç«¶åˆåˆ†æ: '{keywords}'\n\n"
            result_text += "ğŸ“ˆ å¸‚å ´åˆ†æ:\n"
            result_text += "â€¢ ç«¶åˆä½œå“æ•°: å¤šæ•°ã‚ã‚Šï¼ˆæ¿€æˆ¦åŒºï¼‰\n"
            result_text += "â€¢ å¹³å‡ä¾¡æ ¼: 800-1200å††\n"
            result_text += "â€¢ å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ: ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ãƒ»ç”»è³ªãƒ»ãƒœãƒªãƒ¥ãƒ¼ãƒ \n\n"
            result_text += "ğŸ’¡ å‹åˆ©æˆ¦ç•¥:\n"
            result_text += "â€¢ ç‹¬è‡ªè¨­å®šãƒ»ã‚­ãƒ£ãƒ©ã®é­…åŠ›å‘ä¸Š\n"
            result_text += "â€¢ é«˜è§£åƒåº¦ãƒ»ç¾éº—ã‚¤ãƒ©ã‚¹ãƒˆ\n"
            result_text += "â€¢ ãƒœãƒ¼ãƒŠã‚¹è¦ç´ ï¼ˆå·®åˆ†ãƒ»ãŠã¾ã‘ï¼‰\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            return [TextContent(type="text", text=f"âŒ ç«¶åˆåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")]

    async def fanza_tag_extractor(self, ranking_period: str = "weekly", extract_type: str = "tags", limit: int = 100) -> List[TextContent]:
        """å£²ã‚Œç­‹ã‹ã‚‰ã‚¿ã‚°æŠ½å‡º"""
        try:
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‹ã‚‰äººæ°—ä½œå“ã®ã‚¿ã‚°ã‚’æŠ½å‡º
            result_text = f"ğŸ·ï¸ å£²ã‚Œç­‹ã‚¿ã‚°åˆ†æ ({ranking_period})\n\n"
            result_text += "ğŸ”¥ äººæ°—ã‚¿ã‚°TOP20:\n"
            
            popular_tags = [
                "ç¾å°‘å¥³", "å­¦åœ’", "åˆ¶æœ", "JK", "å·¨ä¹³", "ãƒ„ãƒ³ãƒ‡ãƒ¬", 
                "æ‹æ„›", "ç´”æ„›", "åˆä½“é¨“", "æ”¾èª²å¾Œ", "éƒ¨æ´»", "å¹¼é¦´æŸ“",
                "ãŠå§‰ã•ã‚“", "äººå¦»", "OL", "ãƒ¡ã‚¤ãƒ‰", "ãƒŠãƒ¼ã‚¹", "å…ˆç”Ÿ",
                "ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼", "ç•°ä¸–ç•Œ"
            ]
            
            for i, tag in enumerate(popular_tags, 1):
                result_text += f"{i:2d}. #{tag}\n"
            
            result_text += "\nğŸ’° é«˜å˜ä¾¡ã‚¸ãƒ£ãƒ³ãƒ«:\n"
            result_text += "â€¢ ãƒœã‚¤ã‚¹ä»˜ãCGé›†: 1000-2000å††\n"
            result_text += "â€¢ ã‚²ãƒ¼ãƒ å½¢å¼: 1500-3000å††\n"
            result_text += "â€¢ ãƒãƒ³ã‚¬ï¼ˆé•·ç·¨ï¼‰: 800-1500å††\n"
            
            return [TextContent(type="text", text=result_text)]
            
        except Exception as e:
            return [TextContent(type="text", text=f"âŒ ã‚¿ã‚°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")]

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    async with FanzaMCPServer() as server:
        from mcp.server.stdio import stdio_server
        
        logger.info("ğŸš€ FANZA MCP Server èµ·å‹•ä¸­...")
        await stdio_server(server.app)

if __name__ == "__main__":
    asyncio.run(main())