#!/usr/bin/env python3
"""
è¨˜éŒ²æ•´ç†ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
====================
è“„ç©ã•ã‚ŒãŸè¨˜éŒ²ã®ç¶™ç¶šçš„æ•´ç†ãƒ»æœ€é©åŒ–ãƒ»ç®¡ç†

æ©Ÿèƒ½:
- é‡è¤‡è¨˜éŒ²ã®é™¤å»
- å¤ã„è¨˜éŒ²ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
- è¨˜éŒ²ã®è¦ç´„ãƒ»åœ§ç¸®
- æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ç›£è¦–
- é‡è¦åº¦ã«ã‚ˆã‚‹åˆ†é¡
"""

import os
import json
import shutil
import hashlib
import gzip
from datetime import datetime, timedelta
from pathlib import Path
import threading
import time
import subprocess

class RecordOptimizer:
    def __init__(self):
        self.tool_path = Path("/mnt/c/Claude Code/tool")
        self.obsidian_path = Path("G:/ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–/Obsidian Vault")
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.archive_dir = self.tool_path / "record_archive"
        self.compressed_dir = self.archive_dir / "compressed"
        self.index_dir = self.archive_dir / "indices"
        
        # è¨­å®š
        self.max_daily_records = 100  # 1æ—¥æœ€å¤§100è¨˜éŒ²
        self.archive_after_days = 30  # 30æ—¥å¾Œã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        self.compress_after_days = 7   # 7æ—¥å¾Œã«åœ§ç¸®
        self.max_storage_mb = 500     # æœ€å¤§500MB
        
        self.setup_directories()
        self.start_optimization()

    def setup_directories(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ"""
        self.archive_dir.mkdir(exist_ok=True)
        self.compressed_dir.mkdir(exist_ok=True)
        self.index_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ è¨˜éŒ²æ•´ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {self.archive_dir}")

    def start_optimization(self):
        """è¨˜éŒ²æ•´ç†é–‹å§‹"""
        print("ğŸ”„ è¨˜éŒ²æ•´ç†ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        
        # åˆå›ã®å…¨ä½“æ•´ç†
        self.perform_full_optimization()
        
        # ç¶™ç¶šçš„ç›£è¦–é–‹å§‹
        self.start_continuous_monitoring()

    def perform_full_optimization(self):
        """å…¨ä½“æœ€é©åŒ–å®Ÿè¡Œ"""
        print("ğŸ§¹ è¨˜éŒ²å…¨ä½“æœ€é©åŒ–å®Ÿè¡Œä¸­...")
        
        # 1. é‡è¤‡é™¤å»
        duplicates_removed = self.remove_duplicates()
        
        # 2. å¤ã„è¨˜éŒ²ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        archived_count = self.archive_old_records()
        
        # 3. è¨˜éŒ²ã®åœ§ç¸® (ç„¡åŠ¹åŒ–)
        compressed_count = 0  # åœ§ç¸®ã¯ç„¡åŠ¹
        
        # 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        self.create_search_index()
        
        # 5. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        storage_info = self.check_storage_usage()
        
        # çµæœè¨˜éŒ²
        optimization_result = {
            "timestamp": datetime.now().isoformat(),
            "duplicates_removed": duplicates_removed,
            "archived_count": archived_count,
            "compressed_count": compressed_count,
            "storage_info": storage_info
        }
        
        self.log_optimization_result(optimization_result)
        
        print(f"âœ… å…¨ä½“æœ€é©åŒ–å®Œäº†: é‡è¤‡é™¤å»{duplicates_removed}ä»¶, ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–{archived_count}ä»¶")

    def remove_duplicates(self):
        """é‡è¤‡è¨˜éŒ²é™¤å»"""
        print("ğŸ” é‡è¤‡è¨˜éŒ²æ¤œå‡ºãƒ»é™¤å»ä¸­...")
        
        duplicates_removed = 0
        seen_hashes = set()
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        for json_file in self.tool_path.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if content_hash in seen_hashes:
                    # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    backup_path = self.archive_dir / f"duplicate_{json_file.name}"
                    shutil.move(json_file, backup_path)
                    duplicates_removed += 1
                    print(f"  é‡è¤‡é™¤å»: {json_file.name}")
                else:
                    seen_hashes.add(content_hash)
                    
            except Exception as e:
                print(f"âš ï¸ é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ {json_file}: {e}")
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡è¡Œé™¤å»
        for log_file in self.tool_path.glob("*.log"):
            try:
                duplicates_removed += self.remove_duplicate_lines(log_file)
            except Exception as e:
                print(f"âš ï¸ ãƒ­ã‚°é‡è¤‡é™¤å»ã‚¨ãƒ©ãƒ¼ {log_file}: {e}")
        
        return duplicates_removed

    def remove_duplicate_lines(self, log_file):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡è¡Œé™¤å»"""
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            unique_lines = []
            seen_lines = set()
            removed_count = 0
            
            for line in lines:
                line_hash = hashlib.md5(line.strip().encode()).hexdigest()
                if line_hash not in seen_lines:
                    unique_lines.append(line)
                    seen_lines.add(line_hash)
                else:
                    removed_count += 1
            
            if removed_count > 0:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                backup_path = self.archive_dir / f"{log_file.name}.backup"
                shutil.copy2(log_file, backup_path)
                
                # é‡è¤‡é™¤å»ç‰ˆã§ä¸Šæ›¸ã
                with open(log_file, 'w', encoding='utf-8') as f:
                    f.writelines(unique_lines)
                
                print(f"  {log_file.name}: {removed_count}è¡Œã®é‡è¤‡é™¤å»")
            
            return removed_count
            
        except Exception as e:
            print(f"âš ï¸ {log_file}ã®é‡è¤‡è¡Œé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def archive_old_records(self):
        """å¤ã„è¨˜éŒ²ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        print("ğŸ“¦ å¤ã„è¨˜éŒ²ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸­...")
        
        cutoff_date = datetime.now() - timedelta(days=self.archive_after_days)
        archived_count = 0
        
        # å¤ã„JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        for json_file in self.tool_path.glob("*.json"):
            try:
                file_mtime = datetime.fromtimestamp(json_file.stat().st_mtime)
                
                if file_mtime < cutoff_date:
                    archive_path = self.archive_dir / json_file.name
                    shutil.move(json_file, archive_path)
                    archived_count += 1
                    print(f"  ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {json_file.name}")
                    
            except Exception as e:
                print(f"âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ {json_file}: {e}")
        
        # å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        for log_file in self.tool_path.glob("*.log"):
            try:
                file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                
                if file_mtime < cutoff_date and log_file.stat().st_size > 1024 * 1024:  # 1MBä»¥ä¸Š
                    archive_path = self.archive_dir / log_file.name
                    shutil.move(log_file, archive_path)
                    archived_count += 1
                    print(f"  å¤§å®¹é‡ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {log_file.name}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ­ã‚°ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ {log_file}: {e}")
        
        return archived_count

    def compress_old_records(self):
        """å¤ã„è¨˜éŒ²ã®åœ§ç¸® (ç„¡åŠ¹åŒ–)"""
        print("ğŸš« åœ§ç¸®æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        return 0  # åœ§ç¸®ã¯å®Ÿè¡Œã—ãªã„

    def create_search_index(self):
        """æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ"""
        print("ğŸ” æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆä¸­...")
        
        try:
            index_data = {
                "created": datetime.now().isoformat(),
                "files": {},
                "keywords": {},
                "daily_summary": {}
            }
            
            # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
            for json_file in self.tool_path.glob("*.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                    
                    file_info = {
                        "size": json_file.stat().st_size,
                        "modified": datetime.fromtimestamp(json_file.stat().st_mtime).isoformat(),
                        "type": "session" if "session" in json_file.name else "data",
                        "activities_count": len(content.get("activities", [])) if isinstance(content, dict) else 0
                    }
                    
                    index_data["files"][json_file.name] = file_info
                    
                except Exception as e:
                    print(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã‚¨ãƒ©ãƒ¼ {json_file}: {e}")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
            index_file = self.index_dir / f"search_index_{datetime.now().strftime('%Y%m%d')}.json"
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {index_file}")
            
        except Exception as e:
            print(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

    def check_storage_usage(self):
        """ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯"""
        total_size = 0
        file_count = 0
        
        # tool ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚µã‚¤ã‚º
        for file_path in self.tool_path.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
        
        storage_info = {
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "file_count": file_count,
            "max_size_mb": self.max_storage_mb,
            "usage_percent": round((total_size / (1024 * 1024)) / self.max_storage_mb * 100, 1)
        }
        
        print(f"ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡: {storage_info['total_size_mb']}MB ({storage_info['usage_percent']}%)")
        
        # ä½¿ç”¨é‡ãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã®è­¦å‘Š
        if storage_info["usage_percent"] > 80:
            print(f"âš ï¸ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ãŒ{storage_info['usage_percent']}%ã«é”ã—ã¦ã„ã¾ã™")
            self.emergency_cleanup()
        
        return storage_info

    def emergency_cleanup(self):
        """ç·Šæ€¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸš¨ ç·Šæ€¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
        
        # å¤§ããªãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¼·åˆ¶ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        for log_file in self.tool_path.glob("*.log"):
            if log_file.stat().st_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                archive_path = self.archive_dir / f"emergency_{log_file.name}"
                shutil.move(log_file, archive_path)
                print(f"  ç·Šæ€¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {log_file.name}")
        
        # å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¼·åˆ¶å‰Šé™¤
        for session_file in self.tool_path.glob("session_*.json"):
            file_age = datetime.now() - datetime.fromtimestamp(session_file.stat().st_mtime)
            if file_age.days > 3:  # 3æ—¥ä»¥ä¸Šå¤ã„
                session_file.unlink()
                print(f"  å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤: {session_file.name}")

    def log_optimization_result(self, result):
        """æœ€é©åŒ–çµæœè¨˜éŒ²"""
        try:
            log_file = self.tool_path / "record_optimization.log"
            
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"{json.dumps(result, ensure_ascii=False)}\n")
            
            # Obsidianã«ã‚‚è¨˜éŒ²
            self.log_to_obsidian(result)
            
        except Exception as e:
            print(f"âš ï¸ æœ€é©åŒ–çµæœè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def log_to_obsidian(self, result):
        """Obsidianè¨˜éŒ²"""
        try:
            today = datetime.now().strftime("%Y%m%d")
            obsidian_file = f"G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\Obsidian Vault\\Record_Optimization_{today}.md"
            
            optimization_record = f"""
## {datetime.now().strftime('%H:%M:%S')} - è¨˜éŒ²æœ€é©åŒ–å®Ÿè¡Œ

**å®Ÿè¡Œçµæœ:**
- é‡è¤‡é™¤å»: {result['duplicates_removed']}ä»¶
- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {result['archived_count']}ä»¶  
- åœ§ç¸®: {result['compressed_count']}ä»¶
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡: {result['storage_info']['total_size_mb']}MB ({result['storage_info']['usage_percent']}%)

**åŠ¹æœ:**
- è¨˜éŒ²ã®å“è³ªå‘ä¸Š
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡åŒ–
- æ¤œç´¢æ€§èƒ½å‘ä¸Š

---
"""
            
            ps_command = f'''
            Add-Content -Path "{obsidian_file}" -Value @"
{optimization_record}
"@ -Encoding UTF8
            '''
            
            subprocess.run([
                "powershell.exe", "-Command", ps_command
            ], capture_output=True, timeout=30)
            
        except Exception as e:
            print(f"âš ï¸ Obsidianè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def start_continuous_monitoring(self):
        """ç¶™ç¶šçš„ç›£è¦–é–‹å§‹"""
        print("ğŸ”„ ç¶™ç¶šçš„è¨˜éŒ²ç›£è¦–é–‹å§‹...")
        
        def monitoring_loop():
            while True:
                try:
                    # 1æ™‚é–“ã”ã¨ã®è»½é‡æœ€é©åŒ–
                    if int(time.time()) % 3600 == 0:  # 1æ™‚é–“
                        self.light_optimization()
                    
                    # 24æ™‚é–“ã”ã¨ã®å…¨ä½“æœ€é©åŒ–
                    if int(time.time()) % 86400 == 0:  # 24æ™‚é–“
                        self.perform_full_optimization()
                    
                    time.sleep(300)  # 5åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯
                    
                except Exception as e:
                    print(f"âš ï¸ ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(300)
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç›£è¦–é–‹å§‹
        monitor_thread = threading.Thread(target=monitoring_loop, daemon=True)
        monitor_thread.start()

    def light_optimization(self):
        """è»½é‡æœ€é©åŒ–"""
        try:
            # é‡è¤‡é™¤å»ã®ã¿å®Ÿè¡Œ
            duplicates = self.remove_duplicates()
            if duplicates > 0:
                print(f"ğŸ§¹ è»½é‡æœ€é©åŒ–: {duplicates}ä»¶ã®é‡è¤‡é™¤å»")
                
        except Exception as e:
            print(f"âš ï¸ è»½é‡æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ”§ è¨˜éŒ²æ•´ç†ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    optimizer = RecordOptimizer()
    
    try:
        print("ğŸ“ è¨˜éŒ²æ•´ç†ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­... (Ctrl+C ã§åœæ­¢)")
        while True:
            time.sleep(60)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ è¨˜éŒ²æ•´ç†ã‚·ã‚¹ãƒ†ãƒ åœæ­¢")

if __name__ == "__main__":
    main()